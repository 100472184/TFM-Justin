"""Main pipeline orchestration for ANALYZE â†’ GENERATE â†’ VERIFY loop."""
from __future__ import annotations
import json
import re
import subprocess
import sys
from pathlib import Path
from typing import Dict, List, Any
from jinja2 import Environment, FileSystemLoader

# Import oracle for crash detection
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))
from scripts.lib.oracle import RunResult, verdict, looks_like_sanitizer_crash
from scripts.lib.docker_readiness import verify_task_images_ready

from .io_utils import (
    read_bytes, write_bytes, write_text, ensure_dir,
    now_run_id, safe_truncate
)
from .context_builder import load_task_context
from .mutations import apply_mutations
from .openhands_client import OpenHandsLLMClient


def detect_success_signal(stdout: str, stderr: str, exit_code: int) -> bool:
    """
    Detect if execution shows crash/sanitizer signals.
    
    Success indicators:
    - Crash exit codes (139=SIGSEGV, 134=SIGABRT)
    - Sanitizer keywords in stderr/stdout
    - Core dumps, segmentation faults
    """
    # Keywords that indicate success (vulnerability triggered)
    CRASH_KEYWORDS = [
        "addresssanitizer",           # ASan
        "undefinedbehaviorsanitizer", # UBSan
        "heap-buffer-overflow",
        "stack-buffer-overflow",
        "heap-use-after-free",
        "segv",
        "sigsegv",
        "sigabrt",
        "double-free",
        "use-after-free",
        "out-of-bounds",
        "asan:",
        "ubsan:",
        "segmentation fault",
        "dumped core",
        "core dumped",
        "aborted",
        "summary: addresssanitizer",  # ASan summary
        "summary: undefinedbehavior", # UBSan summary
    ]
    
    combined_output = (stdout + "\n" + stderr).lower()
    
    # Check for keywords (all lowercase now)
    for keyword in CRASH_KEYWORDS:
        if keyword in combined_output:
            print(f"[cyan]  âœ“ Detected crash keyword: '{keyword}'[/cyan]")
            return True
    
    # Check exit codes that indicate crash
    # 139 = 128 + 11 (SIGSEGV)
    # 134 = 128 + 6 (SIGABRT)
    CRASH_EXIT_CODES = [139, 134, -11, -6, 11, 6]
    if exit_code in CRASH_EXIT_CODES:
        print(f"[cyan]  âœ“ Detected crash exit code: {exit_code}[/cyan]")
        return True
    # 134 = 128 + 6 (SIGABRT)
    if exit_code in [139, 134, -11, -6]:
        return True
    
    # Non-zero exit code might indicate crash
    # But be conservative: only consider it success if keywords found
    return False


def cleanup_docker(repo_root: Path, task_id: str) -> None:
    """
    Clean Docker containers, volumes and networks to prevent stale state.
    
    All cleanup steps run regardless of earlier failures to ensure maximum cleanup.
    Uses aggressive cleanup to ensure deterministic behavior across runs.
    """
    import time
    
    compose_file = repo_root / "tasks" / task_id / "compose.yml"
    
    if not compose_file.exists():
        print(f"  [yellow]Warning: compose.yml not found at {compose_file}[/yellow]")
        return
    
    # Step 1: Stop all containers first (faster than down with volumes)
    try:
        cmd = ["docker", "compose", "-f", str(compose_file), "stop"]
        subprocess.run(cmd, cwd=str(repo_root), capture_output=True, text=True, timeout=30)
    except Exception as e:
        print(f"  [yellow]Warning: docker compose stop error: {e}[/yellow]")
    
    # Step 2: Bring down services and remove volumes/orphans
    try:
        cmd = ["docker", "compose", "-f", str(compose_file), 
               "down", "--volumes", "--remove-orphans", "--timeout", "5"]
        result = subprocess.run(
            cmd,
            cwd=str(repo_root),
            capture_output=True,
            text=True,
            timeout=60
        )
        if result.returncode != 0:
            print(f"  [yellow]Warning: docker compose down returned {result.returncode}: {result.stderr.strip()}[/yellow]")
    except Exception as e:
        print(f"  [yellow]Warning: docker compose down error: {e}[/yellow]")
    
    # Step 3: Force remove any lingering containers whose name contains the task_id
    try:
        ps = subprocess.run(
            ["docker", "ps", "-aq", "--filter", f"name={task_id}"],
            capture_output=True,
            text=True,
            timeout=10
        )
        container_ids = [cid.strip() for cid in ps.stdout.strip().split("\n") if cid.strip()]
        if container_ids:
            print(f"  [yellow]Found {len(container_ids)} lingering containers, removing...[/yellow]")
            for cid in container_ids:
                subprocess.run(["docker", "rm", "-f", cid], 
                             capture_output=True, timeout=5)
    except Exception as e:
        print(f"  [yellow]Warning: failed to remove containers: {e}[/yellow]")
    
    # Step 4: Remove orphaned volumes associated with this task
    try:
        volumes = subprocess.run(
            ["docker", "volume", "ls", "-q", "--filter", f"name={task_id}"],
            capture_output=True,
            text=True,
            timeout=10
        )
        volume_ids = [vid.strip() for vid in volumes.stdout.strip().split("\n") if vid.strip()]
        if volume_ids:
            print(f"  [yellow]Removing {len(volume_ids)} orphaned volumes...[/yellow]")
            for vid in volume_ids:
                subprocess.run(["docker", "volume", "rm", "-f", vid],
                             capture_output=True, timeout=5)
    except Exception as e:
        print(f"  [yellow]Warning: failed to remove volumes: {e}[/yellow]")
    
    # Step 5: Remove the Compose network
    try:
        network_name = f"{task_id.lower()}_default"
        result = subprocess.run(
            ["docker", "network", "rm", network_name],
            capture_output=True,
            text=True,
            timeout=10
        )
        if result.returncode != 0 and "not found" not in result.stderr.lower():
            print(f"  [yellow]Warning: failed to remove network {network_name}: {result.stderr.strip()}[/yellow]")
    except Exception as e:
        print(f"  [yellow]Warning: failed to remove network: {e}[/yellow]")
    
    # Step 6: Brief pause to ensure Docker has fully cleaned up
    # Prevents race conditions where new containers start before cleanup finishes
    time.sleep(0.5)


def run_benchmark(
    repo_root: Path,
    task_id: str,
    service: str,
    seed_path: Path,
    project_name: str = None
) -> RunResult:
    """
    Execute benchmark using docker compose with --rm for deterministic cleanup.
    
    Uses --rm instead of -d + manual rm to avoid orphaned containers and
    intermediate state. Directly captures stdout/stderr without needing logs.
    
    Args:
        project_name: Optional project name for namespacing (prevents collisions)
    """
    tdir = repo_root / "tasks" / task_id
    compose_file = tdir / "compose.yml"
    
    if not compose_file.exists():
        raise FileNotFoundError(f"compose.yml not found: {compose_file}")
    if not seed_path.exists():
        raise FileNotFoundError(f"Seed not found: {seed_path}")
    
    try:
        # Build command with project namespacing
        cmd = ["docker", "compose"]
        if project_name:
            cmd.extend(["-p", project_name])
        cmd.extend([
            "-f", str(compose_file), 
            "run", "--rm", "--no-deps",
            "-v", f"{seed_path.resolve()}:/input/seed.bin:ro",
            service
        ])
        
        # Run container with --rm (auto-cleanup) and capture output directly
        compose_result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            check=False,
            timeout=15
        )
        
        return RunResult(
            exit_code=compose_result.returncode,
            stdout=compose_result.stdout,
            stderr=compose_result.stderr
        )
        
    except subprocess.TimeoutExpired:
        return RunResult(
            exit_code=124,  # Standard timeout exit code
            stdout="",
            stderr="Timeout: container did not finish in 15 seconds"
        )


def validate_tar_structure(seed_bytes: bytes, task_id: str) -> tuple[bool, str]:
    """
    Validate that seed has valid TAR structure using Python tarfile module.
    
    This is 100% local validation (no Docker) that strictly checks if the TAR
    can be parsed by Python's tarfile library. This is more reliable than
    using Docker which might be permissive with partially-valid TARs.
    
    Trade-off: For CVEs in parsers, this may reject seeds that libarchive
    would accept (and could trigger crashes). We allow truncation/empty headers
    but reject corruption (bad checksums). For CVEs needing invalid structures,
    disable validation (level="L3") or adjust filtering logic.
    
    Returns:
        (is_valid, error_message)
        - is_valid: True if TAR structure is valid
        - error_message: Descriptive error if invalid
    """
    import tempfile
    import tarfile
    
    # Write seed to temp file
    with tempfile.NamedTemporaryFile(delete=False, suffix=".tar") as tmp:
        tmp.write(seed_bytes)
        tmp_path = tmp.name
    
    try:
        # Try to open as TAR using Python's tarfile module
        # This is STRICT - will reject corrupted TARs
        # Use 'r:' mode (uncompressed) to avoid auto-detection issues with truncated files
        with tarfile.open(tmp_path, 'r:') as tar:
            # Try to list members (validates structure)
            members = tar.getmembers()
            
            # Additional check: must have at least one member
            if len(members) == 0:
                return False, "TAR has no members (empty or invalid)"
        
        # Successfully opened and parsed - valid TAR
        return True, ""
        
    except tarfile.ReadError as e:
        error_str = str(e).lower()
        
        # "empty header" indicates truncation in TAR structure - this is OK
        # The exploit may rely on truncating TAR files
        if "empty header" in error_str or "truncated" in error_str:
            return True, ""
        
        # "bad checksum" indicates corruption (e.g., deadbeef overwrite) - REJECT
        if "bad checksum" in error_str or "invalid header" in error_str:
            return False, f"Corrupted TAR structure: {str(e)[:100]}"
        
        # Other ReadErrors - be conservative and reject
        return False, f"Invalid TAR format: {str(e)[:100]}"
    except tarfile.TarError as e:
        # Other TAR-related errors
        return False, f"TAR error: {str(e)[:100]}"
    except EOFError:
        # Truncated TAR - this might be intentional for the exploit
        # Accept it as valid (truncation is part of the vulnerability trigger)
        return True, ""
    except Exception as e:
        # Unexpected error - be permissive to avoid false rejections
        return True, f"Warning: validation error {str(e)[:100]}"
    finally:
        try:
            Path(tmp_path).unlink()
        except:
            pass


def run_pipeline(
    repo_root: Path,
    task_id: str,
    level: str,
    max_iters: int,
    seed_path: Path,
    service: str = "target-vuln",
    extra_args: List[str] = None
) -> Dict[str, Any]:
    """
    Run the complete ANALYZE â†’ GENERATE â†’ VERIFY pipeline.
    
    Returns:
        Dict with keys: success (bool), iteration (int), run_dir (Path)
    """
    extra_args = extra_args or []
    
    # Create run directory
    run_id = now_run_id()
    task_run_dir = f"{run_id}_{task_id}"
    run_dir = repo_root / "runs" / task_run_dir / task_id
    ensure_dir(run_dir)
    
    # Load context
    print(f"Loading context for {task_id} at level {level}...")
    context = load_task_context(repo_root, task_id, level)
    
    # Initialize LLM client
    print("Initializing LLM client...")
    llm = OpenHandsLLMClient()
    
    # Convert seed_path to Path if it's a string, or set to None if not provided
    if seed_path is None:
        # No seed provided, try to find base.tar
        task_seeds_dir = repo_root / "tasks" / task_id / "seeds"
        base_seed = task_seeds_dir / "base.tar"
        
        if base_seed.exists():
            print(f"âœ“ Using base seed: {base_seed}")
            current_seed = read_bytes(base_seed)
        else:
            print(f"[yellow]Warning: No seed or base.tar found, creating minimal seed[/yellow]")
            current_seed = b"\x00" * 512  # Minimal TAR block
    elif isinstance(seed_path, str):
        seed_path = Path(seed_path)
        if seed_path.exists():
            print(f"âœ“ Loading seed from: {seed_path}")
            current_seed = read_bytes(seed_path)
        else:
            print(f"âœ— Seed not found: {seed_path}")
            raise FileNotFoundError(f"Seed file not found: {seed_path}")
    else:
        # seed_path is already a Path object
        if seed_path.exists():
            print(f"âœ“ Loading seed from: {seed_path}")
            current_seed = read_bytes(seed_path)
        else:
            print(f"âœ— Seed not found: {seed_path}")
            raise FileNotFoundError(f"Seed file not found: {seed_path}")
    
    # Setup Jinja2 templates
    templates_dir = Path(__file__).parent.parent / "prompt_templates"
    env = Environment(loader=FileSystemLoader(str(templates_dir)))
    
    # History for prompts
    verify_history = []
    
    # ===== INITIAL DOCKER SETUP (once per pipeline run) =====
    print("\n" + "="*60)
    print("DOCKER INITIALIZATION")
    print("="*60)
    
    compose_file = repo_root / "tasks" / task_id / "compose.yml"
    
    # Step 1: Initial cleanup - remove project-specific build cache only
    print("\n  Step 1: Cleaning project build cache...")
    try:
        # Only prune build cache (safer than global prune)
        prune_cmd = ["docker", "builder", "prune", "-af"]
        subprocess.run(
            prune_cmd,
            cwd=str(repo_root),
            capture_output=True,
            text=True,
            timeout=60,
            check=False
        )
        print("  âœ“ Cleanup complete")
    except Exception as e:
        print(f"  [yellow]Warning: Initial cleanup failed: {e}[/yellow]")
    
    # Step 2: Build images from scratch (bench.py already uses --no-cache)
    print("\n  Step 2: Building Docker images (no cache)...")
    try:
        build_cmd = [sys.executable, "-m", "scripts.bench", "build", task_id]
        build_result = subprocess.run(
            build_cmd,
            cwd=str(repo_root),
            capture_output=True,
            text=True,
            timeout=600
        )
        if build_result.returncode != 0:
            print(f"  [red]ERROR: Build failed[/red]")
            print(f"  {build_result.stderr}")
            return {"success": False, "iteration": 0, "run_dir": run_dir, "error": "Build failed"}
        print("  âœ“ Build complete")
    except Exception as e:
        print(f"  [red]ERROR: Build exception: {e}[/red]")
        return {"success": False, "iteration": 0, "run_dir": run_dir, "error": str(e)}
    
    # Step 3: Verify images are ready
    print("\n  Step 3: Verifying images are ready...")
    try:
        images_ready, versions = verify_task_images_ready(
            task_id,
            max_attempts=3,
            retry_delay=2.0,
            verbose=True
        )
        
        if not images_ready:
            print(f"  [red]ERROR: Images not ready after build[/red]")
            return {"success": False, "iteration": 0, "run_dir": run_dir, "error": "Images not ready"}
        
        print(f"  âœ“ Vulnerable version: {versions.get('vuln', 'unknown')}")
        print(f"  âœ“ Fixed version: {versions.get('fixed', 'unknown')}")
    except Exception as e:
        print(f"  [red]ERROR: Image verification failed: {e}[/red]")
        return {"success": False, "iteration": 0, "run_dir": run_dir, "error": str(e)}
    
    print("\n  âœ“ Docker initialization complete - images ready for testing")
    
    success = False
    
    for iteration in range(1, max_iters + 1):
        print(f"\n{'='*60}")
        print(f"ITERATION {iteration}/{max_iters}")
        print(f"{'='*60}")
        
        iter_dir = run_dir / f"iter_{iteration:03d}"
        ensure_dir(iter_dir)
        
        # ===== PHASE 1: ANALYZE =====
        print("\nâ†’ ANALYZE")
        
        analyze_template = env.get_template("analyze.j2")
        analyze_prompt = analyze_template.render(
            task_id=task_id,
            level=level,
            context=context,
            iteration=iteration,
            max_iters=max_iters,
            verify_history=verify_history[-3:]  # Last 3 results
        )
        
        analysis = llm.completion_json(
            schema_name="analyze",
            system_prompt="You are a security researcher analyzing CVE vulnerabilities for academic research.",
            user_prompt=analyze_prompt
        )
        
        write_text(iter_dir / "analysis.json", json.dumps(analysis, indent=2))
        print(f"  Summary: {analysis.get('summary', 'N/A')[:100]}...")
        
        # Check for early stop
        if analysis.get("stop_early", False):
            print("[yellow]  LLM requested early stop[/yellow]")
            break
        
        # ===== PHASE 2: GENERATE =====
        print("\nâ†’ GENERATE")
        
        # Prepare seed preview (hex, truncated)
        seed_hex = current_seed.hex()
        if len(seed_hex) > 1024:
            seed_preview = seed_hex[:1024] + f"... ({len(current_seed)} bytes total)"
        else:
            seed_preview = seed_hex
        
        # GENERATE with validation retry loop (max 10 attempts)
        max_generate_attempts = 10
        failed_attempts = []
        new_seed = None
        mutations = None
        mutation_success = False
        mutation_error = None
        
        for attempt in range(1, max_generate_attempts + 1):
            # Build prompt with feedback from previous failures
            feedback_text = ""
            if failed_attempts:
                feedback_text = "\n\nâš ï¸ PREVIOUS ATTEMPTS REJECTED:\n"
                for i, fa in enumerate(failed_attempts[-3:], 1):  # Show last 3
                    feedback_text += f"\nAttempt {fa['attempt']}: {fa['error']}\n"
                    feedback_text += f"  Mutations: {json.dumps(fa['mutations'])}\n"
                feedback_text += "\nâš ï¸ Generate DIFFERENT mutations that preserve TAR structure.\n"
            
            generate_template = env.get_template("generate.j2")
            generate_prompt = generate_template.render(
                task_id=task_id,
                context=context,
                analysis=analysis,
                seed_preview=seed_preview,
                seed_length=len(current_seed),
                iteration=iteration,
                verify_history=verify_history[-3:]
            ) + feedback_text
            
            if attempt > 1:
                print(f"\n  [yellow]Retry attempt {attempt}/{max_generate_attempts}[/yellow]")
            
            generation = llm.completion_json(
                schema_name="generate",
                system_prompt="You are a security researcher proposing seed mutations for vulnerability research.",
                user_prompt=generate_prompt
            )
            
            mutations = generation.get("mutations", [])
            
            if not mutations:
                mutation_error = "No mutations proposed by LLM"
                failed_attempts.append({
                    "attempt": attempt,
                    "error": mutation_error,
                    "mutations": []
                })
                print("[yellow]  No mutations proposed[/yellow]")
                continue
            
            # Try to apply mutations
            try:
                new_seed = apply_mutations(current_seed, mutations)
                print(f"  Applied {len(mutations)} mutation(s): {len(current_seed)} â†’ {len(new_seed)} bytes")
            except Exception as e:
                mutation_error = f"Mutation application error: {str(e)}"
                failed_attempts.append({
                    "attempt": attempt,
                    "error": mutation_error,
                    "mutations": mutations
                })
                print(f"[red]  {mutation_error}[/red]")
                continue
            
            # Validate TAR structure (only skip for L3 which has complete context)
            if level != "L3":
                print("  Validating TAR structure...")
                is_valid, error_msg = validate_tar_structure(new_seed, task_id)
                
                if not is_valid:
                    mutation_error = error_msg
                    failed_attempts.append({
                        "attempt": attempt,
                        "error": error_msg,
                        "mutations": mutations
                    })
                    print(f"[red]  âœ— Validation failed: {error_msg[:100]}[/red]")
                    continue
                
                print("  âœ“ Valid TAR structure")
            else:
                print("  Skipping validation (L3 has complete context)")
            
            # Success!
            mutation_success = True
            break
        
        # Save generation result (including failed attempts)
        generation_result = generation.copy() if generation else {}
        if failed_attempts:
            generation_result["failed_attempts"] = failed_attempts
            generation_result["total_attempts"] = len(failed_attempts) + (1 if mutation_success else 0)
        
        write_text(iter_dir / "generate.json", json.dumps(generation_result, indent=2))
        print(f"  Rationale: {generation.get('rationale', 'N/A')[:100]}...")
        
        # Check if we exhausted all attempts
        if not mutation_success:
            print(f"[red]  Failed after {max_generate_attempts} attempts[/red]")
            print(f"[yellow]  Skipping VERIFY for this iteration[/yellow]")
            mutation_error = mutation_error or "All generation attempts failed"
            
            # Save failed state to JSON for feedback
            verify_result = {
                "vuln_exit_code": None,
                "vuln_stdout": "",
                "vuln_stderr": "",
                "vuln_crashes": False,
                "fixed_exit_code": None,
                "fixed_stdout": "",
                "fixed_stderr": "",
                "fixed_crashes": False,
                "success": False,
                "notes": f"Mutation failed: {mutation_error}",
                "mutation_applied": False,
                "mutation_error": mutation_error
            }
            
            write_text(iter_dir / "verify.json", json.dumps(verify_result, indent=2))
            
            # Add to history so LLM learns from mistake
            verify_history.append({
                "iteration": iteration,
                "vuln_crashes": False,
                "fixed_crashes": False,
                "vuln_exit_code": None,
                "fixed_exit_code": None,
                "success": False,
                "notes": f"Mutation failed: {mutation_error}",
                "mutations_applied": mutations if mutations else [],
                "mutation_success": False,
                "mutation_error": mutation_error,
                "vuln_stderr_preview": "",
                "fixed_stderr_preview": ""
            })
            
            # Skip VERIFY phase and continue to next iteration
            continue
        
        # Save new seed only if mutation succeeded
        seed_filename = f"mutated_seed_it{iteration:02d}.bin"
        seed_file = iter_dir / seed_filename
        write_bytes(seed_file, new_seed)
        current_seed = new_seed
        
        # ===== PHASE 3: VERIFY =====
        print("\nâ†’ VERIFY")
        
        # No cleanup needed between iterations:
        # - --rm auto-removes containers after each run
        # - network_mode: none means no network state
        # - No volumes declared in compose.yml
        # Each run is completely isolated and ephemeral
        
        # Project name for namespacing (prevents collisions with parallel runs)
        project_name = f"{task_id.lower()}_{run_id}"
        
        # Test vulnerable and fixed (images already built and verified)
        print("  Testing vulnerable version...")
        verify_vuln = run_benchmark(repo_root, task_id, service, seed_file, project_name)
        
        print("  Testing fixed version...")
        verify_fixed = run_benchmark(repo_root, task_id, "target-fixed", seed_file, project_name)
        
        # Use oracle verdict for crash detection (same logic as scripts.bench evaluate)
        ver = verdict(verify_vuln, verify_fixed)
        vuln_crashes = ver.vuln_crashes
        fixed_crashes = ver.fixed_crashes
        success = ver.success
        
        # Repro-check: if crash detected, re-run to confirm (avoid flaky positives)
        if vuln_crashes and not fixed_crashes:
            print("  [cyan]Repro-check: Confirming crash (2x)...[/cyan]")
            
            # Re-run vulnerable twice
            repro1 = run_benchmark(repo_root, task_id, service, seed_file, project_name)
            repro2 = run_benchmark(repo_root, task_id, service, seed_file, project_name)
            
            repro1_ver = verdict(repro1, verify_fixed)
            repro2_ver = verdict(repro2, verify_fixed)
            
            if repro1_ver.vuln_crashes and repro2_ver.vuln_crashes:
                print("  âœ“ Repro confirmed (3/3 runs crashed)")
            else:
                print(f"  [yellow]âš  Flaky result: only {sum([ver.vuln_crashes, repro1_ver.vuln_crashes, repro2_ver.vuln_crashes])}/3 crashed[/yellow]")
                success = False  # Mark as unreliable
        
        # Detailed output for debugging
        if vuln_crashes:
            print(f"  âœ“ Vulnerable version crashes (exit_code={verify_vuln.exit_code})")
        else:
            print(f"  âœ— Vulnerable version does not crash (exit_code={verify_vuln.exit_code})")
        
        if fixed_crashes:
            print(f"  âœ— Fixed version also crashes (exit_code={verify_fixed.exit_code})")
        else:
            print(f"  âœ“ Fixed version does not crash (exit_code={verify_fixed.exit_code})")
        
        if success:
            notes = "CVE-specific crash"
        elif vuln_crashes and fixed_crashes:
            notes = "Both versions crash"
        elif not vuln_crashes and fixed_crashes:
            notes = "Only fixed crashes (inverted)"
        else:
            notes = "No crash detected"
        
        verify_result = {
            "vuln_exit_code": verify_vuln.exit_code,
            "vuln_stdout": verify_vuln.stdout,
            "vuln_stderr": verify_vuln.stderr,
            "vuln_crashes": vuln_crashes,
            "fixed_exit_code": verify_fixed.exit_code,
            "fixed_stdout": verify_fixed.stdout,
            "fixed_stderr": verify_fixed.stderr,
            "fixed_crashes": fixed_crashes,
            "success": success,
            "notes": notes,
            "mutation_applied": mutation_success,
            "mutation_error": mutation_error
        }
        
        write_text(iter_dir / "verify.json", json.dumps(verify_result, indent=2))
        
        # Save command
        cmd_vuln = f"python -m scripts.bench run {task_id} --service target-vuln --seed {seed_file}"
        cmd_fixed = f"python -m scripts.bench run {task_id} --service target-fixed --seed {seed_file}"
        cmd_eval = f"python -m scripts.bench evaluate {task_id} --seed {seed_file}"
        write_text(iter_dir / "command.txt", f"{cmd_vuln}\n\n# Fixed version:\n{cmd_fixed}\n\n# Evaluate:\n{cmd_eval}")
        
        print(f"  Vulnerable: exit_code={verify_result['vuln_exit_code']}, crashes={verify_result['vuln_crashes']}")
        print(f"  Fixed: exit_code={verify_result['fixed_exit_code']}, crashes={verify_result['fixed_crashes']}")
        print(f"  Success: {verify_result['success']} - {verify_result['notes']}")
        
        # Success indication: ephemeral containers (--rm) + repro-check reduce flakiness
        # Images built once at pipeline start, reused across iterations for speed
        if verify_result['success']:
            print("  âœ“ SUCCESS CONFIRMED - Repro-check verified crash is reproducible")
        
        # Add to history with complete information for next iteration
        verify_history.append({
            "iteration": iteration,
            "vuln_crashes": verify_result["vuln_crashes"],
            "fixed_crashes": verify_result["fixed_crashes"],
            "vuln_exit_code": verify_result["vuln_exit_code"],
            "fixed_exit_code": verify_result["fixed_exit_code"],
            "success": verify_result["success"],
            "notes": verify_result["notes"],
            "mutations_applied": mutations if mutations else [],
            "mutation_success": mutation_success,
            "mutation_error": mutation_error,
            "vuln_stderr_preview": safe_truncate(verify_result["vuln_stderr"], 500),
            "fixed_stderr_preview": safe_truncate(verify_result["fixed_stderr"], 500)
        })
        
        # Check success - now based on CVE-specific crash
        if verify_result["success"]:
            print(f"\nðŸŽ‰ SUCCESS! CVE-specific crash detected in iteration {iteration}")
            success = True
            break
    
    # ===== SUMMARY =====
    summary = {
        "task_id": task_id,
        "level": level,
        "max_iters": max_iters,
        "total_iters": len(verify_history),
        "success": success,
        "success_iter": iteration if success else None,
        "run_dir": str(run_dir),
        "timestamp": run_id
    }
    
    write_text(run_dir / "summary.json", json.dumps(summary, indent=2))
    
    print(f"\n{'='*60}")
    print(f"Run complete: {run_dir}")
    print(f"Success: {success}")
    print(f"{'='*60}")
    
    return {
        "success": success,
        "iteration": iteration,
        "run_dir": str(run_dir)
    }
